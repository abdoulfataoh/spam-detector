{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4bbf18d",
   "metadata": {},
   "source": [
    "![logo uvbf](https://drive.google.com/uc?export=download&id=1eP-0JTAV3p7a_mhAPHyVdFiB9pPckKr6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a6690",
   "metadata": {},
   "source": [
    "# Projet: Determiner si un sms est un spam ou pas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b46b688",
   "metadata": {},
   "source": [
    "## Membres du groupe:\n",
    "- KABORE Abdoul Fataoh\n",
    "- OUEDRAOGO Ibrahim Alassane\n",
    "- ROUAMBA Fadilatou\n",
    "- ROBGO Karima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb156edf",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-samba",
   "metadata": {},
   "source": [
    "# Importation & Description des donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8162007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5752de68",
   "metadata": {},
   "source": [
    "- **Importation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "loose-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sms_spams_collection.txt\", sep=\"\\t\", header = None)\n",
    "df.columns = [\"label\", \"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2e5ccf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3322c6af",
   "metadata": {},
   "source": [
    "- **Description**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "228adb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       content\n",
       "label         \n",
       "ham       4825\n",
       "spam       747"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-combat",
   "metadata": {},
   "source": [
    "# Netoyage des donnees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-grade",
   "metadata": {},
   "source": [
    "**Le netoyage consiste a supprimer des informations de notre dataset qui sont pas utile a notre modele**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-median",
   "metadata": {},
   "source": [
    "- **Supprimer les points de ponctuations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "million-wallace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "lesbian-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    text = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "scheduled-husband",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"content_clean\"] = df[\"content\"].apply(lambda x: remove_punctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "productive-closure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>Go until jurong point crazy Available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>Will ü b going to esplanade fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>Pity  was in mood for that Soany other suggest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>The guy did some bitching but I acted like id ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>Rofl Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            content  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...   \n",
       "1      ham                      Ok lar... Joking wif u oni...   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3      ham  U dun say so early hor... U c already then say...   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "...    ...                                                ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...   \n",
       "5568   ham               Will ü b going to esplanade fr home?   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...   \n",
       "5571   ham                         Rofl. Its true to its name   \n",
       "\n",
       "                                          content_clean  \n",
       "0     Go until jurong point crazy Available only in ...  \n",
       "1                               Ok lar Joking wif u oni  \n",
       "2     Free entry in 2 a wkly comp to win FA Cup fina...  \n",
       "3           U dun say so early hor U c already then say  \n",
       "4     Nah I dont think he goes to usf he lives aroun...  \n",
       "...                                                 ...  \n",
       "5567  This is the 2nd time we have tried 2 contact u...  \n",
       "5568                Will ü b going to esplanade fr home  \n",
       "5569  Pity  was in mood for that Soany other suggest...  \n",
       "5570  The guy did some bitching but I acted like id ...  \n",
       "5571                          Rofl Its true to its name  \n",
       "\n",
       "[5572 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-shield",
   "metadata": {},
   "source": [
    "- **Supprimer les stopwords** <br>\n",
    "Les stopwords sont des mots qui n'ont pas de valeurs ajouter a notre modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "future-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "animal-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniser(text):\n",
    "    regex = re.compile(\"\\w+\")\n",
    "    words = re.findall(regex, text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "apparent-reset",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>content_tokeniser</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>Go until jurong point crazy Available only in ...</td>\n",
       "      <td>[Go, until, jurong, point, crazy, Available, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       content_clean  \\\n",
       "0  Go until jurong point crazy Available only in ...   \n",
       "1                            Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        U dun say so early hor U c already then say   \n",
       "4  Nah I dont think he goes to usf he lives aroun...   \n",
       "\n",
       "                                   content_tokeniser  \n",
       "0  [Go, until, jurong, point, crazy, Available, o...  \n",
       "1                     [Ok, lar, Joking, wif, u, oni]  \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...  \n",
       "3  [U, dun, say, so, early, hor, U, c, already, t...  \n",
       "4  [Nah, I, dont, think, he, goes, to, usf, he, l...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"content_tokeniser\"] = df[\"content_clean\"].apply(lambda x: tokeniser(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fewer-mauritius",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "together-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(content_list):\n",
    "    result = [word for word in content_list if word not in nltk.corpus.stopwords.words(\"english\")]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "surrounded-context",
   "metadata": {},
   "outputs": [],
   "source": [
    " df[\"no_stopwords\"]= df[\"content_tokeniser\"].apply(lambda x: remove_stopwords(x))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "instructional-gibraltar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>content_tokeniser</th>\n",
       "      <th>no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>Go until jurong point crazy Available only in ...</td>\n",
       "      <td>[Go, until, jurong, point, crazy, Available, o...</td>\n",
       "      <td>[Go, jurong, point, crazy, Available, bugis, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
       "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[Nah, I, dont, think, goes, usf, lives, around...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>[This, is, the, 2nd, time, we, have, tried, 2,...</td>\n",
       "      <td>[This, 2nd, time, tried, 2, contact, u, U, 750...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>Will ü b going to esplanade fr home</td>\n",
       "      <td>[Will, ü, b, going, to, esplanade, fr, home]</td>\n",
       "      <td>[Will, ü, b, going, esplanade, fr, home]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>Pity  was in mood for that Soany other suggest...</td>\n",
       "      <td>[Pity, was, in, mood, for, that, Soany, other,...</td>\n",
       "      <td>[Pity, mood, Soany, suggestions]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>The guy did some bitching but I acted like id ...</td>\n",
       "      <td>[The, guy, did, some, bitching, but, I, acted,...</td>\n",
       "      <td>[The, guy, bitching, I, acted, like, id, inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>Rofl Its true to its name</td>\n",
       "      <td>[Rofl, Its, true, to, its, name]</td>\n",
       "      <td>[Rofl, Its, true, name]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            content  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...   \n",
       "1      ham                      Ok lar... Joking wif u oni...   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3      ham  U dun say so early hor... U c already then say...   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "...    ...                                                ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...   \n",
       "5568   ham               Will ü b going to esplanade fr home?   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...   \n",
       "5571   ham                         Rofl. Its true to its name   \n",
       "\n",
       "                                          content_clean  \\\n",
       "0     Go until jurong point crazy Available only in ...   \n",
       "1                               Ok lar Joking wif u oni   \n",
       "2     Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3           U dun say so early hor U c already then say   \n",
       "4     Nah I dont think he goes to usf he lives aroun...   \n",
       "...                                                 ...   \n",
       "5567  This is the 2nd time we have tried 2 contact u...   \n",
       "5568                Will ü b going to esplanade fr home   \n",
       "5569  Pity  was in mood for that Soany other suggest...   \n",
       "5570  The guy did some bitching but I acted like id ...   \n",
       "5571                          Rofl Its true to its name   \n",
       "\n",
       "                                      content_tokeniser  \\\n",
       "0     [Go, until, jurong, point, crazy, Available, o...   \n",
       "1                        [Ok, lar, Joking, wif, u, oni]   \n",
       "2     [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3     [U, dun, say, so, early, hor, U, c, already, t...   \n",
       "4     [Nah, I, dont, think, he, goes, to, usf, he, l...   \n",
       "...                                                 ...   \n",
       "5567  [This, is, the, 2nd, time, we, have, tried, 2,...   \n",
       "5568       [Will, ü, b, going, to, esplanade, fr, home]   \n",
       "5569  [Pity, was, in, mood, for, that, Soany, other,...   \n",
       "5570  [The, guy, did, some, bitching, but, I, acted,...   \n",
       "5571                   [Rofl, Its, true, to, its, name]   \n",
       "\n",
       "                                           no_stopwords  \n",
       "0     [Go, jurong, point, crazy, Available, bugis, n...  \n",
       "1                        [Ok, lar, Joking, wif, u, oni]  \n",
       "2     [Free, entry, 2, wkly, comp, win, FA, Cup, fin...  \n",
       "3         [U, dun, say, early, hor, U, c, already, say]  \n",
       "4     [Nah, I, dont, think, goes, usf, lives, around...  \n",
       "...                                                 ...  \n",
       "5567  [This, 2nd, time, tried, 2, contact, u, U, 750...  \n",
       "5568           [Will, ü, b, going, esplanade, fr, home]  \n",
       "5569                   [Pity, mood, Soany, suggestions]  \n",
       "5570  [The, guy, bitching, I, acted, like, id, inter...  \n",
       "5571                            [Rofl, Its, true, name]  \n",
       "\n",
       "[5572 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-turkey",
   "metadata": {},
   "source": [
    "3. Le steamming <br>\n",
    "Le steamming est une operation qui consiste a contracter du texte dans le but de reduire son poids et de le traiter de facon plus rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "perceived-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "swiss-response",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'racin'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"racines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "916160d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'racine'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemmatize(\"racines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aggressive-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(content_list: list) -> list:\n",
    "    result = [ps.stem(sword) for sword in content_list]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "australian-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing(content_list: list) -> list:\n",
    "    result = [wn.lemmatize(lword) for lword in content_list]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dd720c",
   "metadata": {},
   "source": [
    "## **(1) Notre fonction final de netoyage et contraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "94bece72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sms(text: str):\n",
    "    result = remove_punctuations(text)\n",
    "    tokens = tokeniser(result)\n",
    "    text = stemming(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-ocean",
   "metadata": {},
   "source": [
    "# Vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-physiology",
   "metadata": {},
   "source": [
    "Apres l'etape de preparation des donnees, et avant d'appliquer les algorithme de machine learning pour entrainer notre modele, nous devrons proceder a la vectorisation des donnees.\n",
    "Il s'agit de denombre le nombre d'ocurance d'un mot donnee pour chaque document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-georgia",
   "metadata": {},
   "source": [
    "- **CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eight-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "worst-tribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = [\"Nous avons un presentation de nlp. nlp est le traitement auto de langage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bizarre-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vectorisation = CountVectorizer(analyzer=clean_sms)\n",
    "vect_final = full_vectorisation.fit_transform(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "embedded-exposure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auto</th>\n",
       "      <th>avon</th>\n",
       "      <th>de</th>\n",
       "      <th>est</th>\n",
       "      <th>langag</th>\n",
       "      <th>le</th>\n",
       "      <th>nlp</th>\n",
       "      <th>nou</th>\n",
       "      <th>present</th>\n",
       "      <th>traitement</th>\n",
       "      <th>un</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   auto  avon  de  est  langag  le  nlp  nou  present  traitement  un\n",
       "0     1     1   2    1       1   1    2    1        1           1   1"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = full_vectorisation.get_feature_names()\n",
    "d = pd.DataFrame(vect_final.toarray())\n",
    "d.columns = f\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44a39d0",
   "metadata": {},
   "source": [
    "- **TF-IDS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-petite",
   "metadata": {},
   "source": [
    "### TF-IDS: Term Frequency-Inverse Frequency Document Frequency\n",
    "Il s'agit de donner un poids a un **mot** dans un document\n",
    "<p>Le poids d'un mot determine le degree d'utilisation de ce mots dans un document donne par rapport aux autres documents</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa4f14b",
   "metadata": {},
   "source": [
    "## W<sub>i,j</sub> = tf<sub>i,j</sub> * log(N/df<sub>i</sub>)\n",
    "\n",
    "- ### W<sub>i,j</sub> = Le poids du mot i dans le document j\n",
    "- ### tf<sub>i,j</sub>  = Frquence du mot i dans le document j\n",
    "- ### N = Nombre de documents total\n",
    "- ### df = Nombre de document contenant i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "68e032f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2141392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = [\"Nous avons un presentation de nlp. nlp est le traitement auto de langage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bdc382d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_obj = TfidfVectorizer()\n",
    "vectorizer_data = vectorizer_obj.fit_transform(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0397de99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auto</th>\n",
       "      <th>avon</th>\n",
       "      <th>de</th>\n",
       "      <th>est</th>\n",
       "      <th>langag</th>\n",
       "      <th>le</th>\n",
       "      <th>nlp</th>\n",
       "      <th>nou</th>\n",
       "      <th>present</th>\n",
       "      <th>traitement</th>\n",
       "      <th>un</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.242536</td>\n",
       "      <td>0.242536</td>\n",
       "      <td>0.485071</td>\n",
       "      <td>0.242536</td>\n",
       "      <td>0.242536</td>\n",
       "      <td>0.242536</td>\n",
       "      <td>0.485071</td>\n",
       "      <td>0.242536</td>\n",
       "      <td>0.242536</td>\n",
       "      <td>0.242536</td>\n",
       "      <td>0.242536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       auto      avon        de       est    langag        le       nlp  \\\n",
       "0  0.242536  0.242536  0.485071  0.242536  0.242536  0.242536  0.485071   \n",
       "\n",
       "        nou   present  traitement        un  \n",
       "0  0.242536  0.242536    0.242536  0.242536  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = full_vectorisation.get_feature_names()\n",
    "d = pd.DataFrame(vectorizer_data.toarray())\n",
    "d.columns = f\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b2b7e",
   "metadata": {},
   "source": [
    "## **(2) Choix de la technique de vectorisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "99a680f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"sms_spams_collection.txt\", sep=\"\\t\", header = None)\n",
    "data.columns = [\"label\", \"content\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "73d85db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorisation_full = TfidfVectorizer(analyzer=clean_sms)\n",
    "vect_final = vectorisation_full.fit_transform(data['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfeba54",
   "metadata": {},
   "source": [
    "# Feature engenering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b54a9e",
   "metadata": {},
   "source": [
    "<p>Le feature engenering est une etape crutiale dans le machine learning. il permet d'augmenter la puissance explicative d'un jeux de donnees de donnees</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e5b2d",
   "metadata": {},
   "source": [
    "Methode:\n",
    " - soit on ajoute des nouvelles variables\n",
    " - soit on transforme les variables existantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01033a8a",
   "metadata": {},
   "source": [
    "## **(3) Injecter le pourcentage de ponctuation et la longueur du text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "acd16ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_punctuation(text: str):\n",
    "    nb_punc =  sum([1 for c in text if c in string.punctuation])\n",
    "    return round(nb_punc/(len(text)), 4)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6ee6b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_lenght(text: str):\n",
    "    return len(text) - text.count(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "eeb02b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_len'] = data['content'].apply(lambda x: message_lenght(x))\n",
    "data['punctuation_rate'] = data['content'].apply(lambda x: count_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "66876e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>content_len</th>\n",
       "      <th>punctuation_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>92</td>\n",
       "      <td>8.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>24</td>\n",
       "      <td>20.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>128</td>\n",
       "      <td>3.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>39</td>\n",
       "      <td>12.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>49</td>\n",
       "      <td>3.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>131</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>29</td>\n",
       "      <td>2.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>48</td>\n",
       "      <td>12.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>100</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>21</td>\n",
       "      <td>3.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            content  content_len  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...           92   \n",
       "1      ham                      Ok lar... Joking wif u oni...           24   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...          128   \n",
       "3      ham  U dun say so early hor... U c already then say...           39   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...           49   \n",
       "...    ...                                                ...          ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...          131   \n",
       "5568   ham               Will ü b going to esplanade fr home?           29   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...           48   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...          100   \n",
       "5571   ham                         Rofl. Its true to its name           21   \n",
       "\n",
       "      punctuation_rate  \n",
       "0                 8.11  \n",
       "1                20.69  \n",
       "2                 3.87  \n",
       "3                12.24  \n",
       "4                 3.28  \n",
       "...                ...  \n",
       "5567              5.00  \n",
       "5568              2.78  \n",
       "5569             12.28  \n",
       "5570              0.80  \n",
       "5571              3.85  \n",
       "\n",
       "[5572 rows x 4 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e9d98834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>8202</th>\n",
       "      <th>8203</th>\n",
       "      <th>8204</th>\n",
       "      <th>8205</th>\n",
       "      <th>8206</th>\n",
       "      <th>8207</th>\n",
       "      <th>8208</th>\n",
       "      <th>8209</th>\n",
       "      <th>content_len</th>\n",
       "      <th>punctuation_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92</td>\n",
       "      <td>8.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24</td>\n",
       "      <td>20.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128</td>\n",
       "      <td>3.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39</td>\n",
       "      <td>12.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49</td>\n",
       "      <td>3.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>131</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29</td>\n",
       "      <td>2.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48</td>\n",
       "      <td>12.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21</td>\n",
       "      <td>3.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 8212 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3    4    5    6    7    8    9  ...  8202  8203  8204  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   ...   \n",
       "5567  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "5568  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "5569  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "5570  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "5571  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "\n",
       "      8205  8206     8207  8208  8209  content_len  punctuation_rate  \n",
       "0      0.0   0.0  0.00000   0.0   0.0           92              8.11  \n",
       "1      0.0   0.0  0.00000   0.0   0.0           24             20.69  \n",
       "2      0.0   0.0  0.00000   0.0   0.0          128              3.87  \n",
       "3      0.0   0.0  0.00000   0.0   0.0           39             12.24  \n",
       "4      0.0   0.0  0.00000   0.0   0.0           49              3.28  \n",
       "...    ...   ...      ...   ...   ...          ...               ...  \n",
       "5567   0.0   0.0  0.00000   0.0   0.0          131              5.00  \n",
       "5568   0.0   0.0  0.32036   0.0   0.0           29              2.78  \n",
       "5569   0.0   0.0  0.00000   0.0   0.0           48             12.28  \n",
       "5570   0.0   0.0  0.00000   0.0   0.0          100              0.80  \n",
       "5571   0.0   0.0  0.00000   0.0   0.0           21              3.85  \n",
       "\n",
       "[5572 rows x 8212 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = pd.concat([pd.DataFrame(vect_final.toarray()), data['content_len'], data['punctuation_rate']], axis=1)\n",
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f275c6",
   "metadata": {},
   "source": [
    "## 4. Model & entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "169fa5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "87229a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(final_data, data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b8e1f058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn  import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3b533d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "alg_svm= svm.SVC(kernel = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7c2b9273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg_svm.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "76348f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = alg_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "42f937da",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore, _ = score(Y_test, predictions, pos_label='spam', average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "aed5f18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0 / Recall: 0.89 / Accuracy: 0.986\n"
     ]
    }
   ],
   "source": [
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(round(precision, 3),\n",
    "                                                        round(recall, 3),\n",
    "                                                        round((predictions==Y_test).sum() / len(predictions),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b046510d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
